---------- CCC ----------
Login:
ssh shu@dccxl001.pok.ibm.com
ssh shu@dccxl003.pok.ibm.com
ssh shu@dccxl004.pok.ibm.com
ssh shu@dccxl005.pok.ibm.com
ssh shu@dccxl010.pok.ibm.com
ssh shu@dccxl011.pok.ibm.com
ssh shu@dccxl012.pok.ibm.com
ssh shu@dccxl013.pok.ibm.com

Compute:
ssh shu@dccxc004.pok.ibm.com
ssh shu@dccxc201.pok.ibm.com

zz tf12 first activate on login node !!!!!!!!!!!!!!!! It copies envs to compute node, no other way !!!!!!! ->


------ new jobs shu -----

jbsub -interactive -queue x86_7d -mem 40g sh /u/shu/memcached/start.sh 0
jbsub -interactive -queue x86_7d -mem 40g sh /u/shu/memcached/start.sh 4

export PYTHONPATH=/u/shu/anaconda2/envs/tf12/lib/python2.7/site-packages

------ new jobs srallap / mudhakar------

jbsub -interactive -queue x86_7d -mem 40g sh /u/srallap/eeg/memcached/start.sh 0
jbsub -interactive -queue x86_7d -mem 40g sh /u/srallap/eeg/memcached/start.sh 1

export PYTHONPATH=/u/srallap/eeg/anaconda2/envs/tf12/lib/python2.7/site-packages


------- jobs all --------------------

jbsub -interactive -queue x86_24h -mem 10g python 0separateSameLabel.py 

jbsub -interactive -queue x86_6h -mem 20g python 0cache_proc_data.py 0
jbsub -interactive -queue x86_6h -mem 20g python 0cache_proc_data.py 4


jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 1train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 2train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 3train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 4train.py 

jbsub -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 1train.py 
jbsub -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 2train.py 
jbsub -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 3train.py 
jbsub -queue x86_6h -cores 2+1 -mem 20g -require '(cpuf>=40) && (ncpus>=24)' python 4train.py 

jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g python 1train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g python 2train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g python 3train.py 
jbsub -interactive -queue x86_6h -cores 2+1 -mem 20g python 4train.py 



------------ old Job --------------
jbsub -queue x86_6h touch ~/logjobsub

jbsub -interactive -queue x86_6h -mem 20g python ex_csp_motor_imagery.py 1

jbsub -interactive -queue x86_6h -mem 1g python -c "import sys; print(sys.path)"

jbsub -interactive -queue x86_6h which python; python -c "import numpy"

jbsub -interactive -queue x86_6h which python; python -c "import os; print os.environ['LD_LIBRARY_PATH']"
--------

jbsub -interactive -queue x86_6h -mem 20g python runner.py -n
scp -r srallap@dccxl014.pok.ibm.com:/dccstor/srallap1/eeg/bci /dccstor/shu/

--- Git ---
https://github.com/zyrpub/eeg
zyrpub@mail.com
ui*czh*97
--- CCC ---
ssh srallap@dccxl001.pok.ibm.com
/u/srallap/eeg/*
	drwxr-xr-x 20 srallap users 4096 Aug  9 10:14 anaconda2
	drwxr-xr-x  6 srallap users   52 Aug  9 10:47 libmemcached
	-rw-r--r--  1 srallap users  260 Aug  9 12:31 log_cache.txt
	drwxr-xr-x  2 srallap users  118 Aug  9 11:33 mc
	drwxr-xr-x  7 srallap users  134 Aug  9 11:00 memcached
	drwxr-xr-x  3 srallap users   20 Aug  9 11:28 syncdir
/dccstor/srallap1/eeg/*
	drwxr-xr-x 7 srallap users 4096 Aug  8 22:28 bci
	drwxr-sr-x 3 srallap users 4096 Aug 10 09:16 software

ssh shu@dccxl001.pok.ibm.com
/u/shu/*
	drwxr-xr-x 20 shu  4.0K Aug  8 19:40 anaconda2
	drwxr-xr-x  3 shu    40 Aug  8 19:46 .conda
	drwxr-xr-x  6 shu    52 Aug  8 21:19 libmemcached
	drwxr-xr-x  7 shu   134 Aug  8 22:20 memcached
	drwxr-xr-x  6 shu    52 Aug  9 09:10 fox
	drwxr-xr-x  6 shu    52 Aug  9 09:40 gdal
	drwxr-xr-x  6 shu    52 Aug  9 09:41 proj
	drwxr-xr-x  6 shu    89 Aug  9 12:51 xerces
	-rw-r--r--  1 shu  2.6K Aug  9 12:57 .bashrc
	drwxr-xr-x  5 shu    54 Aug  9 13:15 sumo
	drwxr-xr-x  4 shu    34 Aug  9 13:35 syncdir
	drwxr-xr-x  2 shu   118 Aug  9 17:14 mc
/dccstor/shu1/*
	drwxr-xr-x  7 shu  4.0K Aug  8 22:28 bci
	drwxr-xr-x  9 shu  4.0K Aug 10 09:13 software

-----
touch THIS_IS_JUST_FOR_BACK_UP
tar -czvf software_archive.tar.gz software
scp software_archive.tar.gz srallap@dccxl001.pok.ibm.com:/dccstor/srallap1/eeg/software/
==========================
# specifies a job needs 2 cores and 1 GPU.
jbsub -cores 2+1  ls

# specify outputfile stdout
jbsub -out 4train-log.txt cmd

#to launch a job on a compute node with an SSD:
jbsub -r fasttmp <mycmd>
jbsub -interactive -cores 1+1  matlab

# queue: x86_6h 	50 	Open:Active 	200
jbsub -queue x86_6h -mem 2g -out 4train-log.txt ls

# launch only on the machine dccxc050:
jbsub -require "hname=dccxc050" cmd

# stdout err here:
ll ~/.lsf/dcc/
cat ./4train-log.txt

# reserve 1GB by default, this limit is not a hard limit 
jbsub -mem 2g cmd

# queue max core: 'JL/U'
bqueues -l x86_6h

# debug: 
jbsub -interactive  gdb my_prog

# multiple-GPU job may never launch

# mail when job finish
jbsub -mail  ls

export JB_EMAIL_ADDR=shu.zhao@ibm.com


------------- rsync -----------
rsync -avzu --progress --exclude '*.pyc' --exclude-from '0rsync_exclude' ./* zhao97@tarekc55:/home/zhao97/syncdir/<?>

--- once for all servers:
ssh-copy-id shu@dccxl001.pok.ibm.com
jbmon
jbmon -static
jbmon -queue x86_6h
jbinfo
jbadmin -kill id
bqueues
uname -m: x86_64

------------- scp -----------
scp -r ./memcached.conf shu@dccxl001.pok.ibm.com:/u/shu/memcached/

------------- NumPy -------------

NumPy has been installed in the Python installation located at

/opt/share/Python-2.7.9/

To use, set the following environment variables:

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/share/Python-2.7.9/lib

Then, the following should work:

/opt/share/Python-2.7.9/bin/python
import numpy

---- old scipy, no iirnotch  

-------------- Tensorflow --------------
 has been installed in the Python installation located at

/opt/share/Python-2.7.9/

To use, set the following environment variables:

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/lib:/opt/share/Python-2.7.9/lib/
export PATH="/opt/share/Python-2.7.9/bin/:$PATH"


------------- Theano -------------
 has been installed in the Python installation located at

/opt/share/Python-2.7.9/

To use, set the following environment variables:

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/lib:/opt/share/Python-2.7.9/lib/
export PATH="/opt/share/Python-2.7.9/bin/:$PATH"


------------ cuda -------------
# To see all of the versions of cuda installed in /opt/share/:

ls -ld /opt/share/cuda-*

# Here is an example compilation command:

/usr/local/cuda/bin/nvcc -o myprog myprog.cu

# cuDNN has been installed at

/opt/share/cuDNN-v5-7.5
/opt/share/cuda-8.0/x86_64/lib64/libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0
/opt/share/cuDNN-v5.1-8.0/cuda/lib64/libcudnn.so.5
/opt/share/cuDNN-v5-7.5/cuda/lib64/libcudnn.so.5

ll /opt/share/cuda-8.0/x86_64/lib64/ | grep libcuda
ll /usr/local/cuda/lib64/ | grep libcuda
ll /opt/share/cuDNN-v5-7.5/cuda/lib64/ | grep libcuda

-------------- blas ------------
OpenBLAS 0.2.18 is installed at

/opt/share/OpenBLAS-0.2.18/x86_64/include
/opt/share/OpenBLAS-0.2.18/x86_64/lib

/opt/share/OpenBLAS-0.2.14
/opt/share/OpenBLAS-0.2.18/x86_64
/opt/share/OpenBLAS-1.8.18/x86_64

=========== server info

    dccxc[001-180] — Compute nodes. 180 NextScale nx360 m4 machines. Each has 16-24 cores; 256GB RAM; 2x Tesla K40 GPU’s. Half have 2x 800GB SSD’s; half have 1x 6TB HDD’s.

    dccxc[181-183] — Like above (SSD version), except with 2x Tesla K80 GPU’s instead of K40’s.

    dccxc[201-230] — Compute nodes. 30 NextScale nx360 m5 machines. Each has 24 cores; 512GB RAM; 4x Tesla K80 GPU’s. All have 2x 960GB SSD’s.

    dccxl[001-016] — Login nodes. Each has 12 cores; 192GB RAM; no GPU’s.

    -----

    dccpc[001-010] — Compute nodes. 10 PowerSystem S824L machines. Each has 24 cores (Power8); 512GB RAM; 2x Tesla K40 GPU’s; 6x 600GB HDD’s.

    dccpc[201-320] — Compute nodes. 120 PowerSystem Firestone machines. Each has 20 cores (Power8); 512GB RAM; 2x Tesla K80 GPU’s; 2x 1TB HDD’s.

========= yum

yum list installed | grep libevent
yum list installed | grep memcached

----------- CCC login server cofig, memcached
# vim ~/.bashrc
alias ll='ls -alh'
alias mstart='sh /u/shu/memcached/start.sh'
alias mstop='sh /u/shu/memcached/stop.sh'



--- not working:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/lib:/opt/share/Python-2.7.9/lib/
export PATH="/opt/share/Python-2.7.9/bin/:$PATH"




--- i'm using this with tf12 

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64/:/opt/share/cuda-8.0/x86_64/lib64:/usr/lib64/:/usr/lib64/nvidia

LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/opt/share/cuDNN-v5.1-8.0/cuda/lib64/:/opt/share/cuda-8.0/x86_64/lib64:/usr/lib64/:/usr/lib64/nvidia




